{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "### pycode ref: https://github.com/orico/ActiveLearningFrameworkTutorial/blob/master/Active_Learning_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qylS2W3h-3d6"
   },
   "source": [
    "## **Active Learning Tutorial**\n",
    "\n",
    "These days we are exposed to an abundance of unlabeled data either from the Internet or from some other source such as academia or business worlds. Due to the fact that unlabeled data is relatively easy to acquire and is expensive to label, companies usually employ an expert or several employees whose purpose is to label data [1]. Consider the following situation, a data-driven medical company has a lot of MRI scans and they need to employ an expert that will help them interpret these scans. The company has limited resources and they cant interpret or label all of their data; this is the point where they decide to use active-learning (AL). The promise of AL is that by iteratively increasing the size of our carefully selected labeled data, it is possible to achieve similar (or greater [2]) performance to using a fully supervised data-set with a fraction of the cost or time that it takes to label all the data. AL is considered to be a semi-supervised method, between unsupervised and fully supervised in terms of the amount of labeled data, i.e., for unsupervised data we use 0% labeled samples and for fully supervised we use 100% labeled samples. Therefore, the decision of how much data to use or alternatively how much performance is required from the model relies on a resource management decision, in other words it can be a business decision. \n",
    "\n",
    "There are three scenarios for AL: \n",
    "1. Membership query synthesis, i.e., a generated sample is sent to an oracle for labeling.\n",
    "2. Stream-Based selective sampling, i.e, each sample is considered separately - in our case for lable-querying or rejection. Similarly to online-learning, the data is not saved, there are no assumptions on data distribution, and therefore it is adaptive to change. \n",
    "3. Pool-Based sampling, i.e., sampled are chosen from a pool of unlabeled data for the purpose of labeling [3]. \n",
    "In this tutorial we use the third scenario.\n",
    "\n",
    "The following pseudo algorithm represents the learning process, as written in the code, for pool-based sampling:\n",
    "1. Divide the data to a 'pool' and a test-set\n",
    "2. Select 'k' samples from the pool for the initial train-set and label them, the remaining data will be the validation-set\n",
    "3. Normalize all the sets\n",
    "4. Train the Model using the train-set, with balanced weights.\n",
    "5. Use the trained model with the validation-set, get probabilities per sample.\n",
    "6. Use the trained model with the test-set, get performance measures.\n",
    "7. Select 'k' most-informative samples based on per-sample-probabilities, i.e., those that the model was most uncertain about regarding their labelling.\n",
    "8. Move these 'k' samples from the validation set to the train-set and query their labels.\n",
    "9. Inverse normalization for all the data-sets\n",
    "10. Stop according to the stop criterion, otherwise go to 3. \n",
    "\n",
    "There are a few things to note before going forward: \n",
    "1. The fully-supervised performance of a chosen algorithm is usually the upper bound, therefore it is advisable to try several algorithms.\n",
    "2. Normalization for all sets must be inversed and normalized again after we remove samples from the validation set, because our sample distribution changed in both the new validation and new train-sets.\n",
    "2. The sample selection function relies on test-sample probabilities derived from the trained model, therefore we can only use algorithms that provide access to sample probabilities.\n",
    "3. 'k' is a hyper parameter\n",
    "\n",
    "Our most important tool in AL method is the sample selection function, this is the only point where we influence the learning process and it crucial to use the right method. This area is a hot research topic and there are many studies that propose competing selection functions. \n",
    "In this tutorial I propose four known selection functions:\n",
    "1. Random selection - we select 'k' random samples from the validation set.\n",
    "2. Entropy selection - we select 'k' samples with the highest entropy, i.e., with high uncertainty.\n",
    "3. Margin selection - we select 'k' samples with the lowest difference between the two highest class probabilities, i.e., a higher figure will be given for samples whose model was very certain about a single class and lower to samples whose class probabilities are very similar. \n",
    "\n",
    "The code provided [here](https://github.com/orico/ActiveLearningFrameworkTutorial) utilizes a modular architecture in terms of selecting various learning algorithms and selection functions and can be used as a base for other model-function comparisons.\n",
    "\n",
    "We compare several learning algorithms, such as support vector machine (SVM) with a linear kernel, random forest (RF) and logistic regression (LOG). Each algorithm was executed with all of the selection functions using all 'k' = [10,25,50,125,250], accumulating a total of 80 experiments. Due to the random nature of some of the algorithms and the selection functions, it is advisable to run repeated experiments in the code in order to calculate a statistical significant result. However, running times are long and I have chosen to run the experiment only once for each combination of (model,function,k).\n",
    "\n",
    "The following is an explanation of the code and its class architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x19OpFguwPXR"
   },
   "source": [
    "We start with all the needed dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MWwUTInv-oj"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy import stats\n",
    "from pylab import rcParams\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, \\\n",
    "    GradientBoostingClassifier\n",
    "\n",
    "max_queried = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrFIsF1wvzCg"
   },
   "source": [
    "We start by downloading our data and splitting it to train and test, according to known MNIST definitions 60K/10K split. later the train-set will be split to train and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ojzg4Gjwpgh"
   },
   "outputs": [],
   "source": [
    "trainset_size = 60000  # ie., testset_size = 10000\n",
    "\n",
    "def download():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    X = mnist.data.astype('float64')\n",
    "    y = mnist.target\n",
    "    print ('MNIST:', X.shape, y.shape)\n",
    "    return (X, y)\n",
    "\n",
    "\n",
    "def split(train_size):\n",
    "    X_train_full = X[:train_size]\n",
    "    y_train_full = y[:train_size]\n",
    "    X_test = X[train_size:]\n",
    "    y_test = y[train_size:]\n",
    "    return (X_train_full, y_train_full, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrb6IW2ExGST"
   },
   "source": [
    "We create a modular class representation, 'BaseModel' is a base model for the class architecture, you can implement new models and use them interchangeably or in addition to all other models.\n",
    "our current implementations include SVM, logistic regression, random forest and gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2coy8C7xGqS"
   },
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit_predict(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SvmModel(BaseModel):\n",
    "\n",
    "    model_type = 'Support Vector Machine with linear Kernel'\n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training svm...')\n",
    "        self.classifier = SVC(C=1, kernel='linear', probability=True,\n",
    "                              class_weight=c_weight)\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
    "                self.test_y_predicted)\n",
    "\n",
    "\n",
    "class LogModel(BaseModel):\n",
    "\n",
    "    model_type = 'Multinominal Logistic Regression' \n",
    "    \n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training multinomial logistic regression')\n",
    "        train_samples = X_train.shape[0]\n",
    "        self.classifier = LogisticRegression(\n",
    "            C=50. / train_samples,\n",
    "            multi_class='multinomial',\n",
    "            penalty='l1',\n",
    "            solver='saga',\n",
    "            tol=0.1,\n",
    "            class_weight=c_weight,\n",
    "            )\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
    "                self.test_y_predicted)\n",
    "\n",
    "class RfModel(BaseModel):\n",
    "\n",
    "    model_type = 'Random Forest'\n",
    "    \n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training random forest...')\n",
    "        self.classifier = RandomForestClassifier(n_estimators=500, class_weight=c_weight)\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X51UCCeEzqb9"
   },
   "source": [
    "Our 'TrainModel' class accepts one of the previously in defined learning algorithms, trains using the training set and gets performance measurements from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NEcquMhTzqlC"
   },
   "outputs": [],
   "source": [
    "class TrainModel:\n",
    "\n",
    "    def __init__(self, model_object):        \n",
    "        self.accuracies = []\n",
    "        self.model_object = model_object()        \n",
    "\n",
    "    def print_model_type(self):\n",
    "        print (self.model_object.model_type)\n",
    "\n",
    "    # we train normally and get probabilities for the validation set. i.e., we use the probabilities to select the most uncertain samples\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('Train set:', X_train.shape, 'y:', y_train.shape)\n",
    "        print ('Val   set:', X_val.shape)\n",
    "        print ('Test  set:', X_test.shape)\n",
    "        t0 = time.time()\n",
    "        (X_train, X_val, X_test, self.val_y_predicted,\n",
    "         self.test_y_predicted) = \\\n",
    "            self.model_object.fit_predict(X_train, y_train, X_val, X_test, c_weight)\n",
    "        self.run_time = time.time() - t0\n",
    "        return (X_train, X_val, X_test)  # we return them in case we use PCA, with all the other algorithms, this is not needed.\n",
    "\n",
    "    # we want accuracy only for the test set\n",
    "\n",
    "    def get_test_accuracy(self, i, y_test):\n",
    "        classif_rate = np.mean(self.test_y_predicted.ravel() == y_test.ravel()) * 100\n",
    "        self.accuracies.append(classif_rate)               \n",
    "        print('--------------------------------')\n",
    "        print('Iteration:',i)\n",
    "        print('--------------------------------')\n",
    "        print('y-test set:',y_test.shape)\n",
    "        print('Example run in %.3f s' % self.run_time,'\\n')\n",
    "        print(\"Accuracy rate for %f \" % (classif_rate))    \n",
    "        print(\"Classification report for classifier %s:\\n%s\\n\" % (self.model_object.classifier, metrics.classification_report(y_test, self.test_y_predicted)))\n",
    "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, self.test_y_predicted))\n",
    "        print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYar8S_jyYQ9"
   },
   "source": [
    "We create a modular selection function class representation, 'BaseSelectionFunction' is a base class for various sample selection methods. Using this architecture, you can implement new selection methods and use them in addition or instead of previous methods, for experimental purposes. Our current implementations include random-selection, entropy-selection, margin sampling-selection and minimum standard deviation-selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ni-E8tN7yYX2"
   },
   "outputs": [],
   "source": [
    "class BaseSelectionFunction(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def select(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomSelection(BaseSelectionFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        random_state = check_random_state(0)\n",
    "        selection = np.random.choice(probas_val.shape[0], initial_labeled_samples, replace=False)\n",
    "\n",
    "#     print('uniques chosen:',np.unique(selection).shape[0],'<= should be equal to:',initial_labeled_samples)\n",
    "\n",
    "        return selection\n",
    "\n",
    "\n",
    "class EntropySelection(BaseSelectionFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
    "        selection = (np.argsort(e)[::-1])[:initial_labeled_samples]\n",
    "        return selection\n",
    "      \n",
    "class MarginSamplingSelection(BaseSelectionFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        rev = np.sort(probas_val, axis=1)[:, ::-1]\n",
    "        values = rev[:, 0] - rev[:, 1]\n",
    "        selection = np.argsort(values)[:initial_labeled_samples]\n",
    "        return selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ym7QEvD0yI9Z"
   },
   "source": [
    "We have a class that is used to normalize using a MinMax Scaler in the range of [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5aRcKj4D0SQJ"
   },
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \n",
    "    def normalize(self, X_train, X_val, X_test):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val   = self.scaler.transform(X_val)\n",
    "        X_test  = self.scaler.transform(X_test)\n",
    "        return (X_train, X_val, X_test) \n",
    "    \n",
    "    def inverse(self, X_train, X_val, X_test):\n",
    "        X_train = self.scaler.inverse_transform(X_train)\n",
    "        X_val   = self.scaler.inverse_transform(X_val)\n",
    "        X_test  = self.scaler.inverse_transform(X_test)\n",
    "        return (X_train, X_val, X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pl_AfuOx0fkM"
   },
   "source": [
    "Initially we would like to get a random sampling from the unlabeled data-pool, this is done using random.choice without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1GJB6Kb0e54"
   },
   "outputs": [],
   "source": [
    "def get_k_random_samples(initial_labeled_samples, X_train_full,\n",
    "                         y_train_full):\n",
    "    random_state = check_random_state(0)\n",
    "    permutation = np.random.choice(trainset_size,\n",
    "                                   initial_labeled_samples,\n",
    "                                   replace=False)\n",
    "    print ()\n",
    "    print ('initial random chosen samples', permutation.shape),\n",
    "#            permutation)\n",
    "    X_train = X_train_full[permutation]\n",
    "    y_train = y_train_full[permutation]\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    bin_count = np.bincount(y_train.astype('int64'))\n",
    "    unique = np.unique(y_train.astype('int64'))\n",
    "    print (\n",
    "        'initial train set:',\n",
    "        X_train.shape,\n",
    "        y_train.shape,\n",
    "        'unique(labels):',\n",
    "        bin_count,\n",
    "        unique,\n",
    "        )\n",
    "    return (permutation, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeRHks3K0wQO"
   },
   "source": [
    "This is the main class that initiates the active-learning process according to the algorithm described in the introduction. In short, we select 'k' random samples, train a model, select the most informative samples, remove from the validation set, query their labels and retrain using those samples until reaching the stop criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRtOTPPvyJGx"
   },
   "outputs": [],
   "source": [
    "class TheAlgorithm(object):\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    def __init__(self, initial_labeled_samples, model_object, selection_function):\n",
    "        self.initial_labeled_samples = initial_labeled_samples\n",
    "        self.model_object = model_object\n",
    "        self.sample_selection_function = selection_function\n",
    "\n",
    "    def run(self, X_train_full, y_train_full, X_test, y_test):\n",
    "\n",
    "        # initialize process by applying base learner to labeled training data set to obtain Classifier\n",
    "\n",
    "        (permutation, X_train, y_train) = \\\n",
    "            get_k_random_samples(self.initial_labeled_samples,\n",
    "                                 X_train_full, y_train_full)\n",
    "        self.queried = self.initial_labeled_samples\n",
    "        self.samplecount = [self.initial_labeled_samples]\n",
    "\n",
    "        # permutation, X_train, y_train = get_equally_k_random_samples(self.initial_labeled_samples,classes)\n",
    "\n",
    "        # assign the val set the rest of the 'unlabelled' training data\n",
    "\n",
    "        X_val = np.array([])\n",
    "        y_val = np.array([])\n",
    "        X_val = np.copy(X_train_full)\n",
    "        X_val = np.delete(X_val, permutation, axis=0)\n",
    "        y_val = np.copy(y_train_full)\n",
    "        y_val = np.delete(y_val, permutation, axis=0)\n",
    "        print ('val set:', X_val.shape, y_val.shape, permutation.shape)\n",
    "        print ()\n",
    "\n",
    "        # normalize data\n",
    "\n",
    "        normalizer = Normalize()\n",
    "        X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)   \n",
    "        \n",
    "        self.clf_model = TrainModel(self.model_object)\n",
    "        (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
    "        active_iteration = 1\n",
    "        self.clf_model.get_test_accuracy(1, y_test)\n",
    "\n",
    "        # fpfn = self.clf_model.test_y_predicted.ravel() != y_val.ravel()\n",
    "        # print(fpfn)\n",
    "        # self.fpfncount = []\n",
    "        # self.fpfncount.append(fpfn.sum() / y_test.shape[0] * 100)\n",
    "\n",
    "        while self.queried < max_queried:\n",
    "\n",
    "            active_iteration += 1\n",
    "\n",
    "            # get validation probabilities\n",
    "\n",
    "            probas_val = \\\n",
    "                self.clf_model.model_object.classifier.predict_proba(X_val)\n",
    "            print ('val predicted:',\n",
    "                   self.clf_model.val_y_predicted.shape,\n",
    "                   self.clf_model.val_y_predicted)\n",
    "            print ('probabilities:', probas_val.shape, '\\n',\n",
    "                   np.argmax(probas_val, axis=1))\n",
    "\n",
    "            # select samples using a selection function\n",
    "\n",
    "            uncertain_samples = \\\n",
    "                self.sample_selection_function.select(probas_val, self.initial_labeled_samples)\n",
    "\n",
    "            # normalization needs to be inversed and recalculated based on the new train and test set.\n",
    " \n",
    "            X_train, X_val, X_test = normalizer.inverse(X_train, X_val, X_test)   \n",
    "\n",
    "            # get the uncertain samples from the validation set\n",
    "\n",
    "            print ('trainset before', X_train.shape, y_train.shape)\n",
    "            X_train = np.concatenate((X_train, X_val[uncertain_samples]))\n",
    "            y_train = np.concatenate((y_train, y_val[uncertain_samples]))\n",
    "            print ('trainset after', X_train.shape, y_train.shape)\n",
    "            self.samplecount.append(X_train.shape[0])\n",
    "\n",
    "            bin_count = np.bincount(y_train.astype('int64'))\n",
    "            unique = np.unique(y_train.astype('int64'))\n",
    "            print (\n",
    "                'updated train set:',\n",
    "                X_train.shape,\n",
    "                y_train.shape,\n",
    "                'unique(labels):',\n",
    "                bin_count,\n",
    "                unique,\n",
    "                )\n",
    "\n",
    "            X_val = np.delete(X_val, uncertain_samples, axis=0)\n",
    "            y_val = np.delete(y_val, uncertain_samples, axis=0)\n",
    "            print ('val set:', X_val.shape, y_val.shape)\n",
    "            print ()\n",
    "\n",
    "            # normalize again after creating the 'new' train/test sets\n",
    "            normalizer = Normalize()\n",
    "            X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)               \n",
    "\n",
    "            self.queried += self.initial_labeled_samples\n",
    "            (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
    "            self.clf_model.get_test_accuracy(active_iteration, y_test)\n",
    "\n",
    "        print ('final active learning accuracies',\n",
    "               self.clf_model.accuracies)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7OZblsL00L2"
   },
   "source": [
    "We download the data, split to train validation and test, we run the experiment by iterating over all of our training algorithms X all of our selection functions X all possible k's in the range of [10,25,50,125,250]. The accuracy results are kept in a dictionary and pickle-saved to a unique file as soon as the model finishes training - this is crucial when using google colaboratory as it tends to disconnect from time to time. We also limit our training to a maximum of 500 queried samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ main() ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XzrAvJz00dk"
   },
   "outputs": [],
   "source": [
    "(X, y) = download()\n",
    "(X_train_full, y_train_full, X_test, y_test) = split(trainset_size)\n",
    "print ('train:', X_train_full.shape, y_train_full.shape)\n",
    "print ('test :', X_test.shape, y_test.shape)\n",
    "classes = len(np.unique(y))\n",
    "print ('unique classes', classes)\n",
    "\n",
    "def pickle_save(fname, data):\n",
    "  filehandler = open(fname,\"wb\")\n",
    "  pickle.dump(data,filehandler)\n",
    "  filehandler.close() \n",
    "  print('saved', fname, os.getcwd(), os.listdir())\n",
    "\n",
    "def pickle_load(fname):\n",
    "  print(os.getcwd(), os.listdir())\n",
    "  file = open(fname,'rb')\n",
    "  data = pickle.load(file)\n",
    "  file.close()\n",
    "  print(data)\n",
    "  return data\n",
    "  \n",
    "def experiment(d, models, selection_functions, Ks, repeats, contfrom):\n",
    "    algos_temp = []\n",
    "    print ('stopping at:', max_queried)\n",
    "    count = 0\n",
    "    for model_object in models:\n",
    "      if model_object.__name__ not in d:\n",
    "          d[model_object.__name__] = {}\n",
    "      \n",
    "      for selection_function in selection_functions:\n",
    "        if selection_function.__name__ not in d[model_object.__name__]:\n",
    "            d[model_object.__name__][selection_function.__name__] = {}\n",
    "        \n",
    "        for k in Ks:\n",
    "            d[model_object.__name__][selection_function.__name__][str(k)] = []           \n",
    "            \n",
    "            for i in range(0, repeats):\n",
    "                count+=1\n",
    "                if count >= contfrom:\n",
    "                    print ('Count = %s, using model = %s, selection_function = %s, k = %s, iteration = %s.' % (count, model_object.__name__, selection_function.__name__, k, i))\n",
    "                    alg = TheAlgorithm(k, \n",
    "                                       model_object, \n",
    "                                       selection_function\n",
    "                                       )\n",
    "                    alg.run(X_train_full, y_train_full, X_test, y_test)\n",
    "                    d[model_object.__name__][selection_function.__name__][str(k)].append(alg.clf_model.accuracies)\n",
    "                    fname = 'Active-learning-experiment-' + str(count) + '.pkl'\n",
    "                    pickle_save(fname, d)\n",
    "                    if count % 5 == 0:\n",
    "                        print(json.dumps(d, indent=2, sort_keys=True))\n",
    "                    print ()\n",
    "                    print ('---------------------------- FINISHED ---------------------------')\n",
    "                    print ()\n",
    "    return d\n",
    "\n",
    "\n",
    "max_queried = 500 \n",
    "\n",
    "repeats = 1\n",
    "\n",
    "models = [SvmModel, RfModel, LogModel] \n",
    "\n",
    "selection_functions = [RandomSelection, MarginSamplingSelection, EntropySelection] \n",
    "\n",
    "Ks = [250,125,50,25,10] \n",
    "\n",
    "d = {}\n",
    "stopped_at = -1 \n",
    "\n",
    "# print('directory dump including pickle files:', os.getcwd(), np.sort(os.listdir()))  \n",
    "# d = pickle_load('Active-learning-experiment-' + str(stopped_at) + '.pkl')  \n",
    "# print(json.dumps(d, indent=2, sort_keys=True))\n",
    "\n",
    "d = experiment(d, models, selection_functions, Ks, repeats, stopped_at+1)\n",
    "print (d)\n",
    "results = json.loads(json.dumps(d, indent=2, sort_keys=True))\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HT9kYxtP381F"
   },
   "source": [
    "Independently, we trained several models using a train-test split of 60K-10K, the results indicate that the upper-bound for RF, SVM and LOG are 97., 94. and 92.47, respectively.\n",
    "\n",
    "The following graphs show that the random forest classifier paired with the margin-selection method and k=10 is the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1037
    },
    "colab_type": "code",
    "id": "oXoHDRLArYjX",
    "outputId": "91f371fa-abc9-4f30-e297-873906e84191"
   },
   "outputs": [],
   "source": [
    "def performance_plot(fully_supervised_accuracy, dic, models, selection_functions, Ks, repeats):  \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([0,500],[fully_supervised_accuracy, fully_supervised_accuracy],label = 'algorithm-upper-bound')\n",
    "    for model_object in models:\n",
    "      for selection_function in selection_functions:\n",
    "        for idx, k in enumerate(Ks):\n",
    "            x = np.arange(float(Ks[idx]), 500 + float(Ks[idx]), float(Ks[idx]))            \n",
    "            Sum = np.array(dic[model_object][selection_function][k][0])\n",
    "            for i in range(1, repeats):\n",
    "                Sum = Sum + np.array(dic[model_object][selection_function][k][i])\n",
    "            mean = Sum / repeats\n",
    "            ax.plot(x, mean ,label = model_object + '-' + selection_function + '-' + str(k))\n",
    "    ax.legend()\n",
    "    ax.set_xlim([50,500])\n",
    "    ax.set_ylim([40,100])\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "models_str = ['SvmModel', 'RfModel', 'LogModel']\n",
    "selection_functions_str = ['RandomSelection', 'MarginSamplingSelection', 'EntropySelection']\n",
    "Ks_str = ['250','125','50','25','10'] \n",
    "repeats = 1\n",
    "random_forest_upper_bound = 97.\n",
    "svm_upper_bound = 94.\n",
    "log_upper_bound = 92.47\n",
    "total_experiments = len(models_str) * len(selection_functions_str) * len(Ks_str) * repeats\n",
    "\n",
    "print('So which is the better model? under the stopping condition and hyper parameters - random forest is the winner!')\n",
    "performance_plot(random_forest_upper_bound, d, ['RfModel'] , selection_functions_str    , Ks_str, 1)\n",
    "performance_plot(svm_upper_bound, d, ['SvmModel'] , selection_functions_str    , Ks_str, 1)\n",
    "performance_plot(log_upper_bound, d, ['LogModel'] , selection_functions_str    , Ks_str, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 736
    },
    "colab_type": "code",
    "id": "ny0_f1nyOi3H",
    "outputId": "9cb42b48-64d3-406b-a8c0-1e6ef9d94d86"
   },
   "outputs": [],
   "source": [
    "\n",
    "print('So which is the best sample selection function? margin sampling is the winner!')\n",
    "performance_plot(random_forest_upper_bound, d, ['RfModel'], selection_functions_str    , Ks_str, 1)\n",
    "print()\n",
    "print('So which is the best k? k=10 is the winner')\n",
    "performance_plot(random_forest_upper_bound, d, ['RfModel'] , ['MarginSamplingSelection'], Ks_str, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z82imfIIPoJd"
   },
   "source": [
    "I would like to thank Moshe Hadad for his valueable critique regarding PEP8 and Shay Zweig for his proof-reading and comments.\n",
    "\n",
    "Ori Cohen has done his PhD in computer science in the fields of machine learning, brain-computer-interface and neurobiology.\n",
    "\n",
    "[1] Shay Yehezkel, *High Dimensional Statistical Process Control and Application*, M.Sc Thesis.\n",
    "\n",
    "[2] Ilhan, Hamza Osman, and Mehmet Fatih Amasyali. \"[*Active Learning as a Way of Increasing Accuracy*](http://www.ijcte.org/papers/910-AC0013.pdf).\" International Journal of Computer Theory and Engineering 6, no. 6 (2014): 460.\n",
    "\n",
    "[3] Stefan Hosein [*Active Learning: Curious AI Algorithms*](https://www.datacamp.com/community/tutorials/active-learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787655
    },
    "colab_type": "code",
    "id": "HiRcnlZVUpRI",
    "outputId": "07204bd8-e187-4f70-b6d4-337e91dccf43"
   },
   "outputs": [],
   "source": [
    "#@title Executed code for the experiment with output.\n",
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy import stats\n",
    "from pylab import rcParams\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, \\\n",
    "    GradientBoostingClassifier\n",
    "\n",
    "trainset_size = 60000  # ie., testset_size = 10000\n",
    "max_queried = 500\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "def download():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    X = mnist.data.astype('float64')\n",
    "    y = mnist.target\n",
    "    print ('MNIST:', X.shape, y.shape)\n",
    "    return (X, y)\n",
    "\n",
    "\n",
    "def split(train_size):\n",
    "    X_train_full = X[:train_size]\n",
    "    y_train_full = y[:train_size]\n",
    "    X_test = X[train_size:]\n",
    "    y_test = y[train_size:]\n",
    "    return (X_train_full, y_train_full, X_test, y_test)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "class BaseModel(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit_predict(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SvmModel(BaseModel):\n",
    "\n",
    "    model_type = 'Support Vector Machine with linear Kernel'\n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training svm...')\n",
    "        self.classifier = SVC(C=1, kernel='linear', probability=True,\n",
    "                              class_weight=c_weight)\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
    "                self.test_y_predicted)\n",
    "\n",
    "\n",
    "class GmmModel(BaseModel):\n",
    "\n",
    "    model_type = 'Gaussian Mixture Model'\n",
    "    \n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training gaussian mixture model...')\n",
    "        pca = PCA(n_components=75).fit(X_train)  # ,whiten=True).fit(X_train)\n",
    "        reduced_train_data = pca.transform(X_train)\n",
    "        reduced_test_data = pca.transform(X_test)\n",
    "        reduced_val_data = pca.transform(X_val)\n",
    "        print ('PCA: explained_variance_ratio_',\n",
    "               np.sum(pca.explained_variance_ratio_))\n",
    "        self.classifier = GaussianMixture(n_components=10, covariance_type='full')\n",
    "        self.classifier.fit(reduced_train_data)\n",
    "        self.test_y_predicted = \\\n",
    "            self.classifier.predict(reduced_test_data)\n",
    "        self.val_y_predicted = self.classifier.predict(reduced_val_data)\n",
    "        return (reduced_train_data, reduced_val_data,\n",
    "                reduced_test_data, self.val_y_predicted,\n",
    "                self.test_y_predicted)\n",
    "\n",
    "\n",
    "class LogModel(BaseModel):\n",
    "\n",
    "    model_type = 'Multinominal Logistic Regression' \n",
    "    \n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training multinomial logistic regression')\n",
    "        train_samples = X_train.shape[0]\n",
    "        self.classifier = LogisticRegression(\n",
    "            C=50. / train_samples,\n",
    "            multi_class='multinomial',\n",
    "            penalty='l1',\n",
    "            solver='saga',\n",
    "            tol=0.1,\n",
    "            class_weight=c_weight,\n",
    "            )\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
    "                self.test_y_predicted)\n",
    "\n",
    "\n",
    "class GbcModel(BaseModel):\n",
    "\n",
    "    model_type = 'Gradient Boosting Classifier'\n",
    "    \n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training gradient boosting...')\n",
    "        parm = {\n",
    "            'n_estimators': 1200,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.5,\n",
    "            'learning_rate': 0.01,\n",
    "            'min_samples_leaf': 1,\n",
    "            'random_state': 3,\n",
    "            }\n",
    "        self.classifier = GradientBoostingClassifier(**parm)\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
    "                self.test_y_predicted)\n",
    "\n",
    "\n",
    "class RfModel(BaseModel):\n",
    "\n",
    "    model_type = 'Random Forest'\n",
    "    \n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training random forest...')\n",
    "        self.classifier = RandomForestClassifier(n_estimators=500, class_weight=c_weight)\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "class TrainModel:\n",
    "\n",
    "    def __init__(self, model_object):        \n",
    "        self.accuracies = []\n",
    "        self.model_object = model_object()        \n",
    "\n",
    "    def print_model_type(self):\n",
    "        print (self.model_object.model_type)\n",
    "\n",
    "    # we train normally and get probabilities for the validation set. i.e., we use the probabilities to select the most uncertain samples\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('Train set:', X_train.shape, 'y:', y_train.shape)\n",
    "        print ('Val   set:', X_val.shape)\n",
    "        print ('Test  set:', X_test.shape)\n",
    "        t0 = time.time()\n",
    "        (X_train, X_val, X_test, self.val_y_predicted,\n",
    "         self.test_y_predicted) = \\\n",
    "            self.model_object.fit_predict(X_train, y_train, X_val, X_test, c_weight)\n",
    "        self.run_time = time.time() - t0\n",
    "        return (X_train, X_val, X_test)  # we return them in case we use PCA, with all the other algorithms, this is not needed.\n",
    "\n",
    "    # we want accuracy only for the test set\n",
    "\n",
    "    def get_test_accuracy(self, i, y_test):\n",
    "        classif_rate = np.mean(self.test_y_predicted.ravel() == y_test.ravel()) * 100\n",
    "        self.accuracies.append(classif_rate)               \n",
    "        print('--------------------------------')\n",
    "        print('Iteration:',i)\n",
    "        print('--------------------------------')\n",
    "        print('y-test set:',y_test.shape)\n",
    "        print('Example run in %.3f s' % self.run_time,'\\n')\n",
    "        print(\"Accuracy rate for %f \" % (classif_rate))    \n",
    "        print(\"Classification report for classifier %s:\\n%s\\n\" % (self.model_object.classifier, metrics.classification_report(y_test, self.test_y_predicted)))\n",
    "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, self.test_y_predicted))\n",
    "        print('--------------------------------')\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "def get_k_random_samples(initial_labeled_samples, X_train_full,\n",
    "                         y_train_full):\n",
    "    random_state = check_random_state(0)\n",
    "    permutation = np.random.choice(trainset_size,\n",
    "                                   initial_labeled_samples,\n",
    "                                   replace=False)\n",
    "    print ()\n",
    "    print ('initial random chosen samples', permutation.shape),\n",
    "#            permutation)\n",
    "    X_train = X_train_full[permutation]\n",
    "    y_train = y_train_full[permutation]\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    bin_count = np.bincount(y_train.astype('int64'))\n",
    "    unique = np.unique(y_train.astype('int64'))\n",
    "    print (\n",
    "        'initial train set:',\n",
    "        X_train.shape,\n",
    "        y_train.shape,\n",
    "        'unique(labels):',\n",
    "        bin_count,\n",
    "        unique,\n",
    "        )\n",
    "    return (permutation, X_train, y_train)\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "class BaseSelectionFunction(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def select(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomSelection(BaseSelectionFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        random_state = check_random_state(0)\n",
    "        selection = np.random.choice(probas_val.shape[0], initial_labeled_samples, replace=False)\n",
    "\n",
    "#     print('uniques chosen:',np.unique(selection).shape[0],'<= should be equal to:',initial_labeled_samples)\n",
    "\n",
    "        return selection\n",
    "\n",
    "\n",
    "class MinStdSelection(BaseSelectionFunction):\n",
    "\n",
    "    # select the samples where the std is smallest - i.e., there is uncertainty regarding the relevant class\n",
    "    # and then train on these \"hard\" to classify samples.\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        std = np.std(probas_val * 100, axis=1)\n",
    "        selection = std.argsort()[:initial_labeled_samples]\n",
    "        selection = selection.astype('int64')\n",
    "\n",
    "#     print('std',std.shape,std)\n",
    "#     print()\n",
    "#     print('selection',selection, selection.shape, std[selection])\n",
    "\n",
    "        return selection\n",
    "\n",
    "\n",
    "class MarginSamplingSelection(BaseSelectionFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        rev = np.sort(probas_val, axis=1)[:, ::-1]\n",
    "        values = rev[:, 0] - rev[:, 1]\n",
    "        selection = np.argsort(values)[:initial_labeled_samples]\n",
    "        return selection\n",
    "\n",
    "\n",
    "class EntropySelection(BaseSelectionFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
    "        selection = (np.argsort(e)[::-1])[:initial_labeled_samples]\n",
    "        return selection\n",
    "\n",
    "\n",
    "# ====================================================================================================\n",
    "\n",
    "class Normalize(object):\n",
    "    \n",
    "    def normalize(self, X_train, X_val, X_test):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val   = self.scaler.transform(X_val)\n",
    "        X_test  = self.scaler.transform(X_test)\n",
    "        return (X_train, X_val, X_test) \n",
    "    \n",
    "    def inverse(self, X_train, X_val, X_test):\n",
    "        X_train = self.scaler.inverse_transform(X_train)\n",
    "        X_val   = self.scaler.inverse_transform(X_val)\n",
    "        X_test  = self.scaler.inverse_transform(X_test)\n",
    "        return (X_train, X_val, X_test) \n",
    "\n",
    "      \n",
    "# ====================================================================================================\n",
    "\n",
    "class TheAlgorithm(object):\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    def __init__(self, initial_labeled_samples, model_object, selection_function):\n",
    "        self.initial_labeled_samples = initial_labeled_samples\n",
    "        self.model_object = model_object\n",
    "        self.sample_selection_function = selection_function\n",
    "\n",
    "    def run(self, X_train_full, y_train_full, X_test, y_test):\n",
    "\n",
    "        # initialize process by applying base learner to labeled training data set to obtain Classifier\n",
    "\n",
    "        (permutation, X_train, y_train) = \\\n",
    "            get_k_random_samples(self.initial_labeled_samples,\n",
    "                                 X_train_full, y_train_full)\n",
    "        self.queried = self.initial_labeled_samples\n",
    "        self.samplecount = [self.initial_labeled_samples]\n",
    "\n",
    "        # permutation, X_train, y_train = get_equally_k_random_samples(self.initial_labeled_samples,classes)\n",
    "\n",
    "        # assign the val set the rest of the 'unlabelled' training data\n",
    "\n",
    "        X_val = np.array([])\n",
    "        y_val = np.array([])\n",
    "        X_val = np.copy(X_train_full)\n",
    "        X_val = np.delete(X_val, permutation, axis=0)\n",
    "        y_val = np.copy(y_train_full)\n",
    "        y_val = np.delete(y_val, permutation, axis=0)\n",
    "        print ('val set:', X_val.shape, y_val.shape, permutation.shape)\n",
    "        print ()\n",
    "\n",
    "        # normalize data\n",
    "\n",
    "        normalizer = Normalize()\n",
    "        X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)   \n",
    "        \n",
    "        self.clf_model = TrainModel(self.model_object)\n",
    "        (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
    "        active_iteration = 1\n",
    "        self.clf_model.get_test_accuracy(1, y_test)\n",
    "\n",
    "        # fpfn = self.clf_model.test_y_predicted.ravel() != y_val.ravel()\n",
    "        # print(fpfn)\n",
    "        # self.fpfncount = []\n",
    "        # self.fpfncount.append(fpfn.sum() / y_test.shape[0] * 100)\n",
    "\n",
    "        while self.queried < max_queried:\n",
    "\n",
    "            active_iteration += 1\n",
    "\n",
    "            # get validation probabilities\n",
    "\n",
    "            probas_val = \\\n",
    "                self.clf_model.model_object.classifier.predict_proba(X_val)\n",
    "            print ('val predicted:',\n",
    "                   self.clf_model.val_y_predicted.shape,\n",
    "                   self.clf_model.val_y_predicted)\n",
    "            print ('probabilities:', probas_val.shape, '\\n',\n",
    "                   np.argmax(probas_val, axis=1))\n",
    "\n",
    "            # select samples using a selection function\n",
    "\n",
    "            uncertain_samples = \\\n",
    "                self.sample_selection_function.select(probas_val, self.initial_labeled_samples)\n",
    "\n",
    "            # normalization needs to be inversed and recalculated based on the new train and test set.\n",
    " \n",
    "            X_train, X_val, X_test = normalizer.inverse(X_train, X_val, X_test)   \n",
    "\n",
    "            # get the uncertain samples from the validation set\n",
    "\n",
    "            print ('trainset before', X_train.shape, y_train.shape)\n",
    "            X_train = np.concatenate((X_train, X_val[uncertain_samples]))\n",
    "            y_train = np.concatenate((y_train, y_val[uncertain_samples]))\n",
    "            print ('trainset after', X_train.shape, y_train.shape)\n",
    "            self.samplecount.append(X_train.shape[0])\n",
    "\n",
    "            bin_count = np.bincount(y_train.astype('int64'))\n",
    "            unique = np.unique(y_train.astype('int64'))\n",
    "            print (\n",
    "                'updated train set:',\n",
    "                X_train.shape,\n",
    "                y_train.shape,\n",
    "                'unique(labels):',\n",
    "                bin_count,\n",
    "                unique,\n",
    "                )\n",
    "\n",
    "            X_val = np.delete(X_val, uncertain_samples, axis=0)\n",
    "            y_val = np.delete(y_val, uncertain_samples, axis=0)\n",
    "            print ('val set:', X_val.shape, y_val.shape)\n",
    "            print ()\n",
    "\n",
    "            # normalize again after creating the 'new' train/test sets\n",
    "            normalizer = Normalize()\n",
    "            X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)               \n",
    "\n",
    "            self.queried += self.initial_labeled_samples\n",
    "            (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
    "            self.clf_model.get_test_accuracy(active_iteration, y_test)\n",
    "\n",
    "        print ('final active learning accuracies',\n",
    "               self.clf_model.accuracies)\n",
    "\n",
    "\n",
    "# get MNIST\n",
    "\n",
    "(X, y) = download()\n",
    "(X_train_full, y_train_full, X_test, y_test) = split(trainset_size)\n",
    "print ('train:', X_train_full.shape, y_train_full.shape)\n",
    "print ('test :', X_test.shape, y_test.shape)\n",
    "classes = len(np.unique(y))\n",
    "print ('unique classes', classes)\n",
    "\n",
    "def pickle_save(fname, data):\n",
    "  filehandler = open(fname,\"wb\")\n",
    "  pickle.dump(data,filehandler)\n",
    "  filehandler.close() \n",
    "  print('saved', fname, os.getcwd(), os.listdir())\n",
    "\n",
    "def experiment(d, models, selection_functions, Ks, repeats, contfrom):\n",
    "    algos_temp = []\n",
    "    print ('stopping at:', max_queried)\n",
    "    count = 0\n",
    "    for model_object in models:\n",
    "      if model_object.__name__ not in d:\n",
    "          d[model_object.__name__] = {}\n",
    "      \n",
    "      for selection_function in selection_functions:\n",
    "        if selection_function.__name__ not in d[model_object.__name__]:\n",
    "            d[model_object.__name__][selection_function.__name__] = {}\n",
    "        \n",
    "        for k in Ks:\n",
    "            d[model_object.__name__][selection_function.__name__][k] = []           \n",
    "            \n",
    "            for i in range(0, repeats):\n",
    "                count+=1\n",
    "                if count >= contfrom:\n",
    "                    print ('Count = %s, using model = %s, selection_function = %s, k = %s, iteration = %s.' % (count, model_object.__name__, selection_function.__name__, k, i))\n",
    "                    alg = TheAlgorithm(k, \n",
    "                                       model_object, \n",
    "                                       selection_function\n",
    "                                       )\n",
    "                    alg.run(X_train_full, y_train_full, X_test, y_test)\n",
    "                    d[model_object.__name__][selection_function.__name__][k].append(alg.clf_model.accuracies)\n",
    "                    fname = 'Active-learning-experiment-' + str(count) + '.pkl'\n",
    "                    pickle_save(fname, d)\n",
    "                    if count % 5 == 0:\n",
    "                        print(json.dumps(d, indent=2, sort_keys=True))\n",
    "                    print ()\n",
    "                    print ('---------------------------- FINISHED ---------------------------')\n",
    "                    print ()\n",
    "    return d\n",
    "\n",
    "max_queried = 500\n",
    "# max_queried = 20\n",
    "\n",
    "repeats = 1\n",
    "\n",
    "models = [SvmModel, RfModel, LogModel]#, GbcModel]\n",
    "# models = [RfModel, SvmModel]\n",
    "\n",
    "selection_functions = [RandomSelection, MarginSamplingSelection, EntropySelection]#, MinStdSelection]\n",
    "# selection_functions = [MarginSamplingSelection]\n",
    "\n",
    "Ks = [250,125,50,25,10]\n",
    "# Ks = [10]\n",
    "\n",
    "d = {}\n",
    "stopped_at = -1\n",
    "\n",
    "# stopped_at = 73\n",
    "# d = pickle_load('Active-learning-experiment-'+ str(stopped_at) +'.pkl')  \n",
    "# print(json.dumps(d, indent=2, sort_keys=True))\n",
    "\n",
    "d = experiment(d, models, selection_functions, Ks, repeats, stopped_at+1)\n",
    "print(json.dumps(d, indent=2, sort_keys=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jea0AHlpe18z"
   },
   "outputs": [],
   "source": [
    "#@title Manual assignment of the dictionary json dump to a variable, these are the results from the previous cell.\n",
    "\n",
    "d = {\n",
    "  \"LogModel\": {\n",
    "    \"EntropySelection\": {\n",
    "      \"10\": [\n",
    "        [\n",
    "          36.620000000000005,\n",
    "          46.79,\n",
    "          47.49,\n",
    "          49.96,\n",
    "          51.59,\n",
    "          54.25,\n",
    "          49.43,\n",
    "          55.510000000000005,\n",
    "          52.88,\n",
    "          50.89,\n",
    "          53.04,\n",
    "          52.42,\n",
    "          52.410000000000004,\n",
    "          50.949999999999996,\n",
    "          48.699999999999996,\n",
    "          48.89,\n",
    "          51.160000000000004,\n",
    "          47.68,\n",
    "          46.1,\n",
    "          48.91,\n",
    "          50.739999999999995,\n",
    "          53.080000000000005,\n",
    "          54.790000000000006,\n",
    "          50.88,\n",
    "          53.790000000000006,\n",
    "          56.05,\n",
    "          54.06999999999999,\n",
    "          57.85,\n",
    "          54.279999999999994,\n",
    "          55.86,\n",
    "          57.43000000000001,\n",
    "          47.57,\n",
    "          53.480000000000004,\n",
    "          55.21,\n",
    "          53.32,\n",
    "          54.98,\n",
    "          52.51,\n",
    "          54.39000000000001,\n",
    "          55.88999999999999,\n",
    "          54.61,\n",
    "          55.69,\n",
    "          54.510000000000005,\n",
    "          52.559999999999995,\n",
    "          53.66,\n",
    "          56.089999999999996,\n",
    "          54.230000000000004,\n",
    "          51.77,\n",
    "          52.629999999999995,\n",
    "          55.00000000000001,\n",
    "          48.97\n",
    "        ]\n",
    "      ],\n",
    "      \"25\": [\n",
    "        [\n",
    "          59.050000000000004,\n",
    "          52.849999999999994,\n",
    "          54.71,\n",
    "          55.16,\n",
    "          52.349999999999994,\n",
    "          53.94,\n",
    "          53.43,\n",
    "          51.480000000000004,\n",
    "          45.94,\n",
    "          48.92,\n",
    "          48.55,\n",
    "          44.51,\n",
    "          50.72,\n",
    "          48.29,\n",
    "          49.69,\n",
    "          53.61,\n",
    "          49.91,\n",
    "          50.12,\n",
    "          50.59,\n",
    "          52.66\n",
    "        ]\n",
    "      ],\n",
    "      \"50\": [\n",
    "        [\n",
    "          61.82,\n",
    "          60.31999999999999,\n",
    "          54.03,\n",
    "          57.220000000000006,\n",
    "          58.67,\n",
    "          57.11000000000001,\n",
    "          53.18000000000001,\n",
    "          57.76,\n",
    "          55.08,\n",
    "          53.42\n",
    "        ]\n",
    "      ],\n",
    "      \"125\": [\n",
    "        [\n",
    "          69.07,\n",
    "          62.660000000000004,\n",
    "          58.93000000000001,\n",
    "          60.5\n",
    "        ]\n",
    "      ],\n",
    "      \"250\": [\n",
    "        [\n",
    "          73.77,\n",
    "          63.23\n",
    "        ]\n",
    "      ]\n",
    "    },\n",
    "    \"MarginSamplingSelection\": {\n",
    "      \"10\": [\n",
    "        [\n",
    "          33.269999999999996,\n",
    "          56.63,\n",
    "          57.709999999999994,\n",
    "          61.82,\n",
    "          64.57000000000001,\n",
    "          67.43,\n",
    "          69.24,\n",
    "          69.76,\n",
    "          69.87,\n",
    "          71.05,\n",
    "          73.27,\n",
    "          71.35000000000001,\n",
    "          70.75,\n",
    "          72.61,\n",
    "          71.23,\n",
    "          72.57000000000001,\n",
    "          72.38,\n",
    "          73.17,\n",
    "          73.94,\n",
    "          73.44000000000001,\n",
    "          72.26,\n",
    "          72.78,\n",
    "          73.68,\n",
    "          72.75,\n",
    "          73.79,\n",
    "          73.91,\n",
    "          72.39,\n",
    "          71.72,\n",
    "          74.39,\n",
    "          73.09,\n",
    "          73.39,\n",
    "          73.25,\n",
    "          72.8,\n",
    "          74.51,\n",
    "          72.15,\n",
    "          72.08,\n",
    "          71.78,\n",
    "          73.69,\n",
    "          73.19,\n",
    "          73.00999999999999,\n",
    "          72.68,\n",
    "          71.67,\n",
    "          72.71,\n",
    "          74.11,\n",
    "          73.06,\n",
    "          74.33999999999999,\n",
    "          72.56,\n",
    "          72.64,\n",
    "          72.35000000000001,\n",
    "          72.86\n",
    "        ]\n",
    "      ],\n",
    "      \"25\": [\n",
    "        [\n",
    "          41.980000000000004,\n",
    "          52.12,\n",
    "          65.4,\n",
    "          64.84,\n",
    "          65.10000000000001,\n",
    "          64.14,\n",
    "          65.86999999999999,\n",
    "          66.99000000000001,\n",
    "          70.11,\n",
    "          70.44,\n",
    "          67.21000000000001,\n",
    "          71.17,\n",
    "          70.33,\n",
    "          70.12,\n",
    "          71.6,\n",
    "          70.3,\n",
    "          68.16,\n",
    "          69.51,\n",
    "          69.59,\n",
    "          70.85000000000001\n",
    "        ]\n",
    "      ],\n",
    "      \"50\": [\n",
    "        [\n",
    "          62.8,\n",
    "          70.19,\n",
    "          68.39,\n",
    "          71.65,\n",
    "          69.44,\n",
    "          70.94,\n",
    "          70.7,\n",
    "          70.14,\n",
    "          71.17,\n",
    "          70.21\n",
    "        ]\n",
    "      ],\n",
    "      \"125\": [\n",
    "        [\n",
    "          71.11,\n",
    "          72.42,\n",
    "          72.0,\n",
    "          72.81\n",
    "        ]\n",
    "      ],\n",
    "      \"250\": [\n",
    "        [\n",
    "          73.76,\n",
    "          73.76\n",
    "        ]\n",
    "      ]\n",
    "    },\n",
    "    \"RandomSelection\": {\n",
    "      \"10\": [\n",
    "        [\n",
    "          31.61,\n",
    "          50.36000000000001,\n",
    "          53.1,\n",
    "          59.74,\n",
    "          62.7,\n",
    "          64.03999999999999,\n",
    "          64.25,\n",
    "          61.629999999999995,\n",
    "          66.47,\n",
    "          68.0,\n",
    "          65.75999999999999,\n",
    "          65.0,\n",
    "          65.27,\n",
    "          66.46,\n",
    "          66.99000000000001,\n",
    "          66.86,\n",
    "          67.28,\n",
    "          66.72,\n",
    "          67.10000000000001,\n",
    "          68.33,\n",
    "          69.25,\n",
    "          69.69999999999999,\n",
    "          69.32000000000001,\n",
    "          67.72,\n",
    "          69.87,\n",
    "          68.69,\n",
    "          68.83,\n",
    "          70.00999999999999,\n",
    "          69.65,\n",
    "          70.19,\n",
    "          71.41,\n",
    "          70.53,\n",
    "          70.15,\n",
    "          68.74,\n",
    "          70.0,\n",
    "          68.24,\n",
    "          68.92,\n",
    "          67.91,\n",
    "          69.17999999999999,\n",
    "          68.87,\n",
    "          69.02000000000001,\n",
    "          70.00999999999999,\n",
    "          70.47,\n",
    "          71.83,\n",
    "          70.78,\n",
    "          70.61,\n",
    "          71.33,\n",
    "          70.91,\n",
    "          71.0,\n",
    "          71.78999999999999\n",
    "        ]\n",
    "      ],\n",
    "      \"25\": [\n",
    "        [\n",
    "          46.63,\n",
    "          57.86,\n",
    "          62.019999999999996,\n",
    "          70.61,\n",
    "          72.74000000000001,\n",
    "          69.82000000000001,\n",
    "          69.6,\n",
    "          70.81,\n",
    "          70.82000000000001,\n",
    "          69.49,\n",
    "          71.46000000000001,\n",
    "          71.48,\n",
    "          72.04,\n",
    "          72.11,\n",
    "          72.65,\n",
    "          73.09,\n",
    "          73.72,\n",
    "          73.86,\n",
    "          74.69,\n",
    "          73.72999999999999\n",
    "        ]\n",
    "      ],\n",
    "      \"50\": [\n",
    "        [\n",
    "          68.38,\n",
    "          66.14999999999999,\n",
    "          70.45,\n",
    "          72.68,\n",
    "          72.31,\n",
    "          72.33000000000001,\n",
    "          73.44000000000001,\n",
    "          71.78999999999999,\n",
    "          73.11999999999999,\n",
    "          70.84\n",
    "        ]\n",
    "      ],\n",
    "      \"125\": [\n",
    "        [\n",
    "          68.95,\n",
    "          68.53,\n",
    "          71.67,\n",
    "          73.09\n",
    "        ]\n",
    "      ],\n",
    "      \"250\": [\n",
    "        [\n",
    "          73.11,\n",
    "          74.56\n",
    "        ]\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"RfModel\": {\n",
    "    \"EntropySelection\": {\n",
    "      \"10\": [\n",
    "        [\n",
    "          37.519999999999996,\n",
    "          42.53,\n",
    "          51.42,\n",
    "          49.03,\n",
    "          49.25,\n",
    "          52.01,\n",
    "          51.41,\n",
    "          51.89,\n",
    "          53.04,\n",
    "          55.48,\n",
    "          56.989999999999995,\n",
    "          56.720000000000006,\n",
    "          56.93,\n",
    "          55.169999999999995,\n",
    "          56.26,\n",
    "          55.64,\n",
    "          54.769999999999996,\n",
    "          54.98,\n",
    "          54.06999999999999,\n",
    "          53.71,\n",
    "          53.21,\n",
    "          52.800000000000004,\n",
    "          53.54,\n",
    "          52.72,\n",
    "          52.65,\n",
    "          51.849999999999994,\n",
    "          51.67,\n",
    "          51.89,\n",
    "          52.54,\n",
    "          53.03,\n",
    "          56.06,\n",
    "          56.26,\n",
    "          55.669999999999995,\n",
    "          60.17,\n",
    "          60.480000000000004,\n",
    "          59.9,\n",
    "          58.97,\n",
    "          60.019999999999996,\n",
    "          59.81999999999999,\n",
    "          59.86,\n",
    "          59.95,\n",
    "          62.970000000000006,\n",
    "          63.7,\n",
    "          62.94,\n",
    "          62.19,\n",
    "          62.019999999999996,\n",
    "          63.89,\n",
    "          62.150000000000006,\n",
    "          60.17,\n",
    "          62.39\n",
    "        ]\n",
    "      ],\n",
    "      \"25\": [\n",
    "        [\n",
    "          44.21,\n",
    "          39.2,\n",
    "          37.57,\n",
    "          35.82,\n",
    "          36.120000000000005,\n",
    "          33.64,\n",
    "          32.09,\n",
    "          32.1,\n",
    "          32.28,\n",
    "          30.759999999999998,\n",
    "          31.230000000000004,\n",
    "          29.34,\n",
    "          26.419999999999998,\n",
    "          27.05,\n",
    "          26.58,\n",
    "          23.23,\n",
    "          24.0,\n",
    "          23.7,\n",
    "          21.58,\n",
    "          23.84\n",
    "        ]\n",
    "      ],\n",
    "      \"50\": [\n",
    "        [\n",
    "          63.85999999999999,\n",
    "          60.309999999999995,\n",
    "          59.760000000000005,\n",
    "          59.89,\n",
    "          59.160000000000004,\n",
    "          59.519999999999996,\n",
    "          57.9,\n",
    "          59.74,\n",
    "          57.06,\n",
    "          59.91\n",
    "        ]\n",
    "      ],\n",
    "      \"125\": [\n",
    "        [\n",
    "          75.02,\n",
    "          74.36,\n",
    "          72.39999999999999,\n",
    "          72.08\n",
    "        ]\n",
    "      ],\n",
    "      \"250\": [\n",
    "        [\n",
    "          84.39999999999999,\n",
    "          81.82000000000001\n",
    "        ]\n",
    "      ]\n",
    "    },\n",
    "    \"MarginSamplingSelection\": {\n",
    "      \"10\": [\n",
    "        [\n",
    "          25.72,\n",
    "          38.83,\n",
    "          53.99,\n",
    "          57.53,\n",
    "          62.39,\n",
    "          66.58,\n",
    "          64.23,\n",
    "          67.5,\n",
    "          72.75,\n",
    "          76.35,\n",
    "          77.31,\n",
    "          77.96,\n",
    "          78.17,\n",
    "          80.67,\n",
    "          81.76,\n",
    "          83.63000000000001,\n",
    "          85.26,\n",
    "          85.37,\n",
    "          85.82,\n",
    "          85.76,\n",
    "          86.00999999999999,\n",
    "          87.01,\n",
    "          87.45,\n",
    "          87.83,\n",
    "          88.64999999999999,\n",
    "          88.1,\n",
    "          87.92999999999999,\n",
    "          89.21,\n",
    "          89.57000000000001,\n",
    "          90.34,\n",
    "          91.12,\n",
    "          90.86,\n",
    "          91.17,\n",
    "          91.53999999999999,\n",
    "          91.84,\n",
    "          91.75999999999999,\n",
    "          91.07,\n",
    "          91.36,\n",
    "          91.22,\n",
    "          91.88,\n",
    "          92.0,\n",
    "          91.97,\n",
    "          91.7,\n",
    "          92.12,\n",
    "          92.28,\n",
    "          92.57,\n",
    "          92.63,\n",
    "          92.55,\n",
    "          92.80000000000001,\n",
    "          92.75999999999999\n",
    "        ]\n",
    "      ],\n",
    "      \"25\": [\n",
    "        [\n",
    "          39.050000000000004,\n",
    "          51.85999999999999,\n",
    "          67.30000000000001,\n",
    "          74.29,\n",
    "          74.42999999999999,\n",
    "          80.15,\n",
    "          83.55,\n",
    "          83.58,\n",
    "          86.24000000000001,\n",
    "          87.22999999999999,\n",
    "          87.89,\n",
    "          88.42999999999999,\n",
    "          88.94,\n",
    "          90.06,\n",
    "          89.57000000000001,\n",
    "          90.03999999999999,\n",
    "          90.03999999999999,\n",
    "          91.14,\n",
    "          91.86,\n",
    "          91.79\n",
    "        ]\n",
    "      ],\n",
    "      \"50\": [\n",
    "        [\n",
    "          62.99,\n",
    "          75.07000000000001,\n",
    "          75.38,\n",
    "          83.96000000000001,\n",
    "          87.89,\n",
    "          89.62,\n",
    "          89.7,\n",
    "          91.47999999999999,\n",
    "          91.84,\n",
    "          92.49000000000001\n",
    "        ]\n",
    "      ],\n",
    "      \"125\": [\n",
    "        [\n",
    "          76.4,\n",
    "          85.66,\n",
    "          90.22,\n",
    "          92.13\n",
    "        ]\n",
    "      ],\n",
    "      \"250\": [\n",
    "        [\n",
    "          81.26,\n",
    "          87.98\n",
    "        ]\n",
    "      ]\n",
    "    },\n",
    "    \"RandomSelection\": {\n",
    "      \"10\": [\n",
    "        [\n",
    "          36.01,\n",
    "          43.34,\n",
    "          53.94,\n",
    "          58.02,\n",
    "          60.97,\n",
    "          66.44,\n",
    "          68.63,\n",
    "          70.8,\n",
    "          72.08,\n",
    "          73.58,\n",
    "          73.63,\n",
    "          74.0,\n",
    "          74.2,\n",
    "          75.52,\n",
    "          75.41,\n",
    "          75.81,\n",
    "          75.98,\n",
    "          77.97,\n",
    "          77.5,\n",
    "          80.15,\n",
    "          79.81,\n",
    "          81.13,\n",
    "          80.87,\n",
    "          80.58999999999999,\n",
    "          80.86,\n",
    "          81.26,\n",
    "          82.26,\n",
    "          82.44,\n",
    "          82.75,\n",
    "          82.92,\n",
    "          83.76,\n",
    "          83.50999999999999,\n",
    "          85.05,\n",
    "          85.50999999999999,\n",
    "          85.13,\n",
    "          86.02,\n",
    "          86.02,\n",
    "          86.5,\n",
    "          86.09,\n",
    "          85.83,\n",
    "          86.9,\n",
    "          86.46000000000001,\n",
    "          86.38,\n",
    "          86.88,\n",
    "          87.09,\n",
    "          87.87,\n",
    "          87.47,\n",
    "          87.59,\n",
    "          87.74,\n",
    "          87.78\n",
    "        ]\n",
    "      ],\n",
    "      \"25\": [\n",
    "        [\n",
    "          51.64,\n",
    "          60.72,\n",
    "          65.45,\n",
    "          70.28999999999999,\n",
    "          72.94,\n",
    "          76.08,\n",
    "          77.51,\n",
    "          77.78,\n",
    "          79.35,\n",
    "          80.39,\n",
    "          81.6,\n",
    "          81.17,\n",
    "          82.73,\n",
    "          84.28,\n",
    "          84.15,\n",
    "          85.2,\n",
    "          86.13,\n",
    "          86.78,\n",
    "          86.95,\n",
    "          87.59\n",
    "        ]\n",
    "      ],\n",
    "      \"50\": [\n",
    "        [\n",
    "          56.86,\n",
    "          69.43,\n",
    "          71.87,\n",
    "          75.68,\n",
    "          80.01,\n",
    "          82.06,\n",
    "          84.5,\n",
    "          85.92,\n",
    "          86.76,\n",
    "          87.32\n",
    "        ]\n",
    "      ],\n",
    "      \"125\": [\n",
    "        [\n",
    "          71.5,\n",
    "          82.39,\n",
    "          85.76,\n",
    "          87.56\n",
    "        ]\n",
    "      ],\n",
    "      \"250\": [\n",
    "        [\n",
    "          83.19,\n",
    "          88.14999999999999\n",
    "        ]\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"SvmModel\": {\n",
    "    \"EntropySelection\": {\n",
    "      \"10\": [\n",
    "        [\n",
    "          35.93,\n",
    "          35.97,\n",
    "          40.33,\n",
    "          39.39,\n",
    "          41.349999999999994,\n",
    "          42.99,\n",
    "          46.23,\n",
    "          46.18,\n",
    "          47.260000000000005,\n",
    "          52.5,\n",
    "          52.400000000000006,\n",
    "          51.25999999999999,\n",
    "          51.690000000000005,\n",
    "          51.800000000000004,\n",
    "          53.18000000000001,\n",
    "          53.82,\n",
    "          55.88999999999999,\n",
    "          56.120000000000005,\n",
    "          57.269999999999996,\n",
    "          59.41,\n",
    "          59.95,\n",
    "          62.629999999999995,\n",
    "          61.339999999999996,\n",
    "          63.88,\n",
    "          65.34,\n",
    "          65.77,\n",
    "          66.9,\n",
    "          67.96,\n",
    "          68.27,\n",
    "          67.44,\n",
    "          68.45,\n",
    "          68.63,\n",
    "          68.0,\n",
    "          68.47,\n",
    "          68.77,\n",
    "          68.8,\n",
    "          69.17,\n",
    "          68.97,\n",
    "          69.33,\n",
    "          69.67999999999999,\n",
    "          69.95,\n",
    "          70.34,\n",
    "          70.47,\n",
    "          71.19,\n",
    "          71.97,\n",
    "          72.26,\n",
    "          72.06,\n",
    "          71.98,\n",
    "          72.55,\n",
    "          72.78999999999999\n",
    "        ]\n",
    "      ],\n",
    "      \"25\": [\n",
    "        [\n",
    "          50.33,\n",
    "          51.59,\n",
    "          56.599999999999994,\n",
    "          60.24,\n",
    "          61.57,\n",
    "          63.5,\n",
    "          66.74,\n",
    "          68.19,\n",
    "          68.02,\n",
    "          69.8,\n",
    "          75.88000000000001,\n",
    "          77.24,\n",
    "          78.09,\n",
    "          79.38,\n",
    "          80.4,\n",
    "          80.99,\n",
    "          80.28999999999999,\n",
    "          80.12,\n",
    "          79.75999999999999,\n",
    "          80.36999999999999\n",
    "        ]\n",
    "      ],\n",
    "      \"50\": [\n",
    "        [\n",
    "          64.55,\n",
    "          68.75,\n",
    "          71.34,\n",
    "          74.11999999999999,\n",
    "          75.96000000000001,\n",
    "          77.03999999999999,\n",
    "          76.85,\n",
    "          79.19,\n",
    "          80.51,\n",
    "          80.99\n",
    "        ]\n",
    "      ],\n",
    "      \"125\": [\n",
    "        [\n",
    "          78.4,\n",
    "          78.21000000000001,\n",
    "          80.08,\n",
    "          81.0\n",
    "        ]\n",
    "      ],\n",
    "      \"250\": [\n",
    "        [\n",
    "          83.11,\n",
    "          84.53\n",
    "        ]\n",
    "      ]\n",
    "    },\n",
    "    \"MarginSamplingSelection\": {\n",
    "      \"10\": [\n",
    "        [\n",
    "          31.86,\n",
    "          37.6,\n",
    "          46.23,\n",
    "          58.41,\n",
    "          60.34,\n",
    "          65.98,\n",
    "          65.01,\n",
    "          69.86,\n",
    "          72.28999999999999,\n",
    "          74.65,\n",
    "          76.24,\n",
    "          77.37,\n",
    "          77.59,\n",
    "          78.7,\n",
    "          79.06,\n",
    "          80.06,\n",
    "          81.21000000000001,\n",
    "          82.32000000000001,\n",
    "          83.17,\n",
    "          83.78,\n",
    "          84.6,\n",
    "          84.58,\n",
    "          84.47,\n",
    "          85.07000000000001,\n",
    "          85.64,\n",
    "          85.75,\n",
    "          85.7,\n",
    "          86.18,\n",
    "          86.58,\n",
    "          86.50999999999999,\n",
    "          86.71,\n",
    "          87.09,\n",
    "          86.91,\n",
    "          87.13,\n",
    "          87.42999999999999,\n",
    "          87.75,\n",
    "          87.97,\n",
    "          88.23,\n",
    "          88.23,\n",
    "          88.22,\n",
    "          88.16000000000001,\n",
    "          88.03,\n",
    "          88.38000000000001,\n",
    "          88.39,\n",
    "          88.64,\n",
    "          88.98,\n",
    "          89.05999999999999,\n",
    "          89.34,\n",
    "          89.3,\n",
    "          89.52\n",
    "        ]\n",
    "      ],\n",
    "      \"25\": [\n",
    "        [\n",
    "          50.160000000000004,\n",
    "          63.73,\n",
    "          70.14,\n",
    "          74.4,\n",
    "          78.99000000000001,\n",
    "          80.36999999999999,\n",
    "          82.76,\n",
    "          84.35000000000001,\n",
    "          85.78,\n",
    "          86.61999999999999,\n",
    "          87.46000000000001,\n",
    "          87.64,\n",
    "          88.08,\n",
    "          88.75,\n",
    "          88.68,\n",
    "          89.02,\n",
    "          89.39,\n",
    "          89.57000000000001,\n",
    "          89.66,\n",
    "          89.8\n",
    "        ]\n",
    "      ],\n",
    "      \"50\": [\n",
    "        [\n",
    "          59.099999999999994,\n",
    "          69.91000000000001,\n",
    "          75.66000000000001,\n",
    "          81.41000000000001,\n",
    "          82.69,\n",
    "          85.07000000000001,\n",
    "          85.92,\n",
    "          86.89,\n",
    "          87.8,\n",
    "          87.6\n",
    "        ]\n",
    "      ],\n",
    "      \"125\": [\n",
    "        [\n",
    "          79.45,\n",
    "          84.1,\n",
    "          86.36,\n",
    "          88.16000000000001\n",
    "        ]\n",
    "      ],\n",
    "      \"250\": [\n",
    "        [\n",
    "          84.58,\n",
    "          87.38\n",
    "        ]\n",
    "      ]\n",
    "    },\n",
    "    \"RandomSelection\": {\n",
    "      \"10\": [\n",
    "        [\n",
    "          31.009999999999998,\n",
    "          33.54,\n",
    "          48.67,\n",
    "          57.34,\n",
    "          61.68,\n",
    "          64.75999999999999,\n",
    "          69.28,\n",
    "          71.97,\n",
    "          73.2,\n",
    "          74.03,\n",
    "          75.08,\n",
    "          76.01,\n",
    "          76.55,\n",
    "          77.42999999999999,\n",
    "          77.75,\n",
    "          79.72,\n",
    "          80.78,\n",
    "          81.62,\n",
    "          81.8,\n",
    "          82.03,\n",
    "          81.82000000000001,\n",
    "          82.72,\n",
    "          82.94,\n",
    "          83.03,\n",
    "          83.72,\n",
    "          83.81,\n",
    "          83.72,\n",
    "          83.95,\n",
    "          84.38,\n",
    "          85.02,\n",
    "          85.36,\n",
    "          85.65,\n",
    "          85.26,\n",
    "          85.37,\n",
    "          85.37,\n",
    "          85.19,\n",
    "          85.18,\n",
    "          85.61999999999999,\n",
    "          85.47,\n",
    "          85.61999999999999,\n",
    "          85.78,\n",
    "          86.00999999999999,\n",
    "          86.05000000000001,\n",
    "          85.96000000000001,\n",
    "          85.96000000000001,\n",
    "          86.0,\n",
    "          86.24000000000001,\n",
    "          86.6,\n",
    "          86.66,\n",
    "          86.81\n",
    "        ]\n",
    "      ],\n",
    "      \"25\": [\n",
    "        [\n",
    "          51.449999999999996,\n",
    "          67.88,\n",
    "          72.25,\n",
    "          77.82,\n",
    "          79.19,\n",
    "          80.32000000000001,\n",
    "          81.19,\n",
    "          81.91000000000001,\n",
    "          82.67999999999999,\n",
    "          82.89,\n",
    "          82.89,\n",
    "          83.05,\n",
    "          83.91999999999999,\n",
    "          84.69,\n",
    "          84.77,\n",
    "          84.66,\n",
    "          85.0,\n",
    "          85.3,\n",
    "          85.79,\n",
    "          86.08\n",
    "        ]\n",
    "      ],\n",
    "      \"50\": [\n",
    "        [\n",
    "          56.38999999999999,\n",
    "          72.92999999999999,\n",
    "          78.44,\n",
    "          81.0,\n",
    "          82.73,\n",
    "          83.52000000000001,\n",
    "          84.35000000000001,\n",
    "          85.17,\n",
    "          86.17,\n",
    "          86.66\n",
    "        ]\n",
    "      ],\n",
    "      \"125\": [\n",
    "        [\n",
    "          73.76,\n",
    "          83.52000000000001,\n",
    "          85.6,\n",
    "          86.44\n",
    "        ]\n",
    "      ],\n",
    "      \"250\": [\n",
    "        [\n",
    "          83.02000000000001,\n",
    "          85.64\n",
    "        ]\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Active Learning Tutorial",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
