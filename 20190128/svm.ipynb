{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup # text processing\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "# Read labeled training and test data\n",
    "\n",
    "train =pd.read_csv('./all/labeledTrainData.tsv', delimiter='\\t', quoting=3)\n",
    "test= pd.read_csv('./all/testData.tsv',delimiter='\\t', quoting=3)\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詞性的預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a function to preprocess and clean data:\n",
    "#'BeautifulSoup package' used to clean the data removing unwanted HTML.\n",
    "#'Re package' used to remove unwanted punctuations. Few punctuations like '!', '?' and numeric numbers \n",
    "#are not removed as it may be helpful in predicting sementics. \n",
    "#'Tokenizer' used to convert paragraph into array instead of split(). This has improved performace as\n",
    "#it can treat puctuations as separate word. Further steps include 'Stemming' and getting rid of 'stopwords'\n",
    "\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    review_text = BeautifulSoup(raw_review,\"lxml\").get_text()      # remove html\n",
    "    letters = re.sub(\"[^a-zA-Z0-9!?'-]\", \" \", review_text)         # passing only alphabets, numbers and some few punctuations\n",
    "    words_arr=[lemma.lemmatize(w) for w in word_tokenize(str(letters).lower())]   #Lammetize and tokenize\n",
    "    stops = set(stopwords.words(\"english\"))                                 \n",
    "    meaningful_words = [w for w in words_arr if not w in stops]           #removing common english words\n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\\\\"The Classic War of the Worlds\\\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells\\' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\\\"critics\\\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\\\"critics\\\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells\\' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\\\"critics\\\\\" perceive to be its shortcomings.\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's take one example and see the difference\n",
    "train['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classic war world timothy hines entertaining film obviously go great effort length faithfully recreate h g well ' classic book mr hines succeeds watched film appreciated fact wa standard predictable hollywood fare come every year e g spielberg version tom cruise slightest resemblance book obviously everyone look different thing movie envision amateur critic look criticize everything others rate movie important base like entertained people never agree critic enjoyed effort mr hines put faithful h g well ' classic novel found entertaining made easy overlook critic perceive shortcoming\n"
     ]
    }
   ],
   "source": [
    "# cleaned paragraph ex.\n",
    "clean_review = review_to_words( train[\"review\"][1] )\n",
    "print(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Review 25000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# we can proceed to process full training and test data\n",
    "num_reviews = train[\"review\"].size\n",
    "clean_train_reviews = []\n",
    "\n",
    "print (\"Cleaning and parsing the training set movie reviews...\\n\")\n",
    "clean_train_reviews = []\n",
    "for i in range( 0, num_reviews ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print (\"Review %d of %d\" % ( i+1, num_reviews ))                                                                   \n",
    "    clean_train_reviews.append( review_to_words( train['review'][i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Review 1000 of 25000\t\n",
      "Review 2000 of 25000\t\n",
      "Review 3000 of 25000\t\n",
      "Review 4000 of 25000\t\n",
      "Review 5000 of 25000\t\n",
      "Review 6000 of 25000\t\n",
      "Review 7000 of 25000\t\n",
      "Review 8000 of 25000\t\n",
      "Review 9000 of 25000\t\n",
      "Review 10000 of 25000\t\n",
      "Review 11000 of 25000\t\n",
      "Review 12000 of 25000\t\n",
      "Review 13000 of 25000\t\n",
      "Review 14000 of 25000\t\n",
      "Review 15000 of 25000\t\n",
      "Review 16000 of 25000\t\n",
      "Review 17000 of 25000\t\n",
      "Review 18000 of 25000\t\n",
      "Review 19000 of 25000\t\n",
      "Review 20000 of 25000\t\n",
      "Review 21000 of 25000\t\n",
      "Review 22000 of 25000\t\n",
      "Review 23000 of 25000\t\n",
      "Review 24000 of 25000\t\n",
      "Review 25000 of 25000\t\n"
     ]
    }
   ],
   "source": [
    "# Cleaning and Parsing Test Data\n",
    "\n",
    "numOfRev=len(test)\n",
    "clean_test_reviews=[]\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,numOfRev):\n",
    "    if( (i+1) % 1000 == 0 ):\n",
    "        print(\"Review %d of %d\\t\" % (i+1, numOfRev))\n",
    "    clean_review = review_to_words( test[\"review\"][i] )\n",
    "    clean_test_reviews.append(clean_review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer轉換為矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating bag of words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer              #Importing Vectorizer\n",
    "vectorizer= CountVectorizer(analyzer='word',max_features=2500)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)     #Vectorizing training Data\n",
    "train_data_features = train_data_features.toarray() \n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)            #Vectorize Test Data\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用TF-IDF與SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(train_data_features)       #  TFIDF\n",
    "messages_tfidf = tfidf_transformer.transform(train_data_features)\n",
    "test_tfidf=tfidf_transformer.transform(test_data_features)\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(messages_tfidf, train['sentiment'])                  # SVM\n",
    "pred = linear_svc.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.59"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the accuracy\n",
    "acc_linear_svc = round(linear_svc.score(messages_tfidf, train['sentiment']) * 100, 2)\n",
    "acc_linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":pred})\n",
    "final_result.to_csv('output', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
