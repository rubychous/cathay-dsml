{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of reviews are  25000\n",
      "cols are  Index(['id', 'sentiment', 'review'], dtype='object')\n",
      "Sample reviews are \n",
      "                                              review  sentiment\n",
      "0  With all this stuff going down at the moment w...          1\n",
      "1  \\The Classic War of the Worlds\\\" by Timothy Hi...          1\n",
      "2  The film starts with a manager (Nicholas Bell)...          0\n",
      "3  It must be assumed that those who praised this...          0\n",
      "4  Superbly trashy and wondrously unpretentious 8...          1\n",
      "5  I dont know why people think this is such a ba...          1\n"
     ]
    }
   ],
   "source": [
    "#Read the IMDB dataset with 25K reviews for training. \n",
    "\n",
    "df = pd.read_csv(\"./all/labeledTrainData.tsv\", sep = '\\t', \n",
    "                 error_bad_lines=False )\n",
    "print(\"Total no. of reviews are \", df.shape[0])\n",
    "print(\"cols are \", df.columns)\n",
    "print(\"Sample reviews are \")\n",
    "print(df.loc[:5,['review','sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport re\\nimport nltk\\nnltk.download(\\'stopwords\\')\\nfrom nltk.corpus import stopwords\\nfrom nltk.stem.porter import PorterStemmer\\ncorpus = []\\ns = set(stopwords.words(\\'english\\'))\\ns.remove(\\'not\\')\\nprint(\"Stopwords length\", len(s))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the stopwords (common words) to be removed from the corpus\n",
    "\"\"\"\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "s = set(stopwords.words('english'))\n",
    "s.remove('not')\n",
    "print(\"Stopwords length\", len(s))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入外部資料源的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "with open('./all/glove.6B.50d.txt', encoding=\"utf8\") as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將每篇文章做預處理，最後塞入一個向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liouscott/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88583 unique tokens.\n",
      "Total documents  25000\n",
      "max sequence length: 2493\n",
      "min sequence length: 10\n",
      "Shape of data tensor: (25000, 1500)\n",
      "[  1 353 322 ...   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_VCOCAB_SIZE = 5000\n",
    "EMBEDDING_DIM = 50\n",
    "MAX_SEQUENCE_LENGTH = 1500\n",
    "\n",
    "tokenizer = Tokenizer( filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\n",
    "sequences = tokenizer.fit_on_texts(df['review'])\n",
    "word_index = tokenizer.word_index\n",
    "documents = tokenizer.texts_to_sequences(df['review'])\n",
    "#print(word_index)\n",
    "token_count = len(word_index)+1\n",
    "print('Found {} unique tokens.'.format(token_count))\n",
    "\n",
    "#print(t.word_counts)\n",
    "print(\"Total documents \", tokenizer.document_count)\n",
    "#print(t.word_index)\n",
    "#print(t.word_docs)\n",
    "print(\"max sequence length:\", max(len(s) for s in documents))\n",
    "print(\"min sequence length:\", min(len(s) for s in documents))\n",
    "\n",
    "# pad sequences so that we get a N x T matrix\n",
    "data = pad_sequences(documents, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將每個字對應glove引入的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n",
      "Sample embedded dimension \n",
      "[ 0.11891   0.15255  -0.082073 -0.74144   0.75917 ]\n"
     ]
    }
   ],
   "source": [
    "print('Filling pre-trained embeddings...')\n",
    "embedding_matrix = np.zeros((token_count, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "  #if i < MAX_VOCAB_SIZE:\n",
    "    embedding_vector = word2vec.get(word) #get(word) is used instead of [word] as it won't give exception in case word is not found\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i,:] = embedding_vector\n",
    "\n",
    "print(\"Sample embedded dimension \")\n",
    "print(embedding_matrix[10][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, GlobalAveragePooling1D \n",
    "from keras.layers import Embedding, Conv2D, GlobalMaxPooling1D \n",
    "from keras import regularizers\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "  token_count,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=MAX_SEQUENCE_LENGTH,\n",
    "  trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1500, 50)          4429150   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1500, 64)          12864     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 750, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 750, 128)          24704     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 750, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 375, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 375, 256)          65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 375, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 187, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 187, 128)          32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 187, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 187, 64)           8256      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,573,727\n",
      "Trainable params: 144,577\n",
      "Non-trainable params: 4,429,150\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)#, input_shape= (token_count, EMBEDDING_DIM))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 4, padding = 'same', activation='relu'))\n",
    "                 #input_shape=(token_count,EMBEDDING_DIM)))\n",
    "model.add(MaxPooling1D())#kernel_size=500))\n",
    "model.add(Conv1D(filters = 128, kernel_size = 3, padding = 'same',  activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters = 256, kernel_size = 2, padding = 'same', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_1_1/Gather:0\", shape=(?, 1500, 50), dtype=float32)\n",
      "Tensor(\"global_max_pooling1d_4/Max:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"concatenate_1/concat:0\", shape=(?, 300), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1500)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1500, 50)     4429150     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1500, 100)    15100       embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1500, 100)    20100       embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 1500, 100)    25100       embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 100)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 100)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300)          0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 192)          57792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 192)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          24704       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            129         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,572,075\n",
      "Trainable params: 142,925\n",
      "Non-trainable params: 4,429,150\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "x = embedding_layer(inputs)\n",
    "print(x)\n",
    "x1 = Conv1D(filters = 100, kernel_size = 3, padding = 'same', activation='relu'\n",
    "           ,kernel_regularizer=regularizers.l1(0.01))(x)\n",
    "                 #input_shape=(token_count,EMBEDDING_DIM)))\n",
    "x1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "x2 = Conv1D(filters = 100, kernel_size = 4, padding = 'same', activation='relu'\n",
    "           ,kernel_regularizer=regularizers.l1(0.01))(x)\n",
    "                 #input_shape=(token_count,EMBEDDING_DIM)))\n",
    "x2 = GlobalMaxPooling1D()(x2) #pool_size=1500\n",
    "\n",
    "\n",
    "x3 = Conv1D(filters = 100, kernel_size = 5, padding = 'same', activation='relu'\n",
    "           ,kernel_regularizer=regularizers.l1(0.01))(x)\n",
    "                 #input_shape=(token_count,EMBEDDING_DIM)))\n",
    "x3 = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "print(x3)\n",
    "x = Concatenate()([x1,x2,x3])\n",
    "print(x)\n",
    "x = Dense(192)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "Fmodel = Model(inputs=inputs, outputs=output)\n",
    "Fmodel.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(Fmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, df['sentiment'], \n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "#model.fit(x_train, y_train , batch_size=96, epochs=35, validation_split = 0.25)\n",
    "#score = model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/35\n",
      "18000/18000 [==============================] - 331s 18ms/step - loss: 6.8033 - acc: 0.4998 - val_loss: 0.7820 - val_acc: 0.4790\n",
      "Epoch 2/35\n",
      "18000/18000 [==============================] - 329s 18ms/step - loss: 0.7699 - acc: 0.5022 - val_loss: 0.7661 - val_acc: 0.4790\n",
      "Epoch 3/35\n",
      "18000/18000 [==============================] - 322s 18ms/step - loss: 0.7649 - acc: 0.4931 - val_loss: 0.7636 - val_acc: 0.4790\n",
      "Epoch 4/35\n",
      "18000/18000 [==============================] - 320s 18ms/step - loss: 0.7639 - acc: 0.4969 - val_loss: 0.7640 - val_acc: 0.4790\n",
      "Epoch 5/35\n",
      "18000/18000 [==============================] - 327s 18ms/step - loss: 0.7631 - acc: 0.5009 - val_loss: 0.7626 - val_acc: 0.5210\n",
      "Epoch 6/35\n",
      "18000/18000 [==============================] - 345s 19ms/step - loss: 0.7630 - acc: 0.4973 - val_loss: 0.7631 - val_acc: 0.4790\n",
      "Epoch 7/35\n",
      "18000/18000 [==============================] - 338s 19ms/step - loss: 0.7626 - acc: 0.4994 - val_loss: 0.7628 - val_acc: 0.4790\n",
      "Epoch 8/35\n",
      "18000/18000 [==============================] - 344s 19ms/step - loss: 0.7622 - acc: 0.4998 - val_loss: 0.7630 - val_acc: 0.4790\n",
      "Epoch 9/35\n",
      "18000/18000 [==============================] - 340s 19ms/step - loss: 0.7621 - acc: 0.4962 - val_loss: 0.7615 - val_acc: 0.5210\n",
      "Epoch 10/35\n",
      "18000/18000 [==============================] - 328s 18ms/step - loss: 0.7620 - acc: 0.5048 - val_loss: 0.7617 - val_acc: 0.5210\n",
      "Epoch 11/35\n",
      "18000/18000 [==============================] - 322s 18ms/step - loss: 0.7621 - acc: 0.4974 - val_loss: 0.7628 - val_acc: 0.4790\n",
      "Epoch 12/35\n",
      "18000/18000 [==============================] - 323s 18ms/step - loss: 0.7616 - acc: 0.5047 - val_loss: 0.7619 - val_acc: 0.4790\n",
      "Epoch 13/35\n",
      "18000/18000 [==============================] - 324s 18ms/step - loss: 0.7616 - acc: 0.4933 - val_loss: 0.7610 - val_acc: 0.5210\n",
      "Epoch 14/35\n",
      "18000/18000 [==============================] - 336s 19ms/step - loss: 0.7613 - acc: 0.4994 - val_loss: 0.7606 - val_acc: 0.5210\n",
      "Epoch 15/35\n",
      "18000/18000 [==============================] - 338s 19ms/step - loss: 0.7610 - acc: 0.4964 - val_loss: 0.7609 - val_acc: 0.4790\n",
      "Epoch 16/35\n",
      "18000/18000 [==============================] - 334s 19ms/step - loss: 0.7608 - acc: 0.5016 - val_loss: 0.7606 - val_acc: 0.5210\n",
      "Epoch 17/35\n",
      "18000/18000 [==============================] - 331s 18ms/step - loss: 0.7609 - acc: 0.4992 - val_loss: 0.7607 - val_acc: 0.4790\n",
      "Epoch 18/35\n",
      "18000/18000 [==============================] - 332s 18ms/step - loss: 0.7610 - acc: 0.4972 - val_loss: 0.7614 - val_acc: 0.4790\n",
      "Epoch 19/35\n",
      "18000/18000 [==============================] - 329s 18ms/step - loss: 0.7608 - acc: 0.4992 - val_loss: 0.7617 - val_acc: 0.4790\n",
      "Epoch 20/35\n",
      "18000/18000 [==============================] - 333s 18ms/step - loss: 0.7608 - acc: 0.4972 - val_loss: 0.7606 - val_acc: 0.4790\n",
      "Epoch 21/35\n",
      "18000/18000 [==============================] - 334s 19ms/step - loss: 0.7609 - acc: 0.5000 - val_loss: 0.7610 - val_acc: 0.4790\n",
      "Epoch 22/35\n",
      "18000/18000 [==============================] - 322s 18ms/step - loss: 0.7607 - acc: 0.4966 - val_loss: 0.7608 - val_acc: 0.4790\n",
      "Epoch 23/35\n",
      "18000/18000 [==============================] - 334s 19ms/step - loss: 0.7605 - acc: 0.4954 - val_loss: 0.7604 - val_acc: 0.4790\n",
      "Epoch 24/35\n",
      "18000/18000 [==============================] - 333s 19ms/step - loss: 0.7605 - acc: 0.4924 - val_loss: 0.7602 - val_acc: 0.4790\n",
      "Epoch 25/35\n",
      "18000/18000 [==============================] - 338s 19ms/step - loss: 0.7604 - acc: 0.5003 - val_loss: 0.7603 - val_acc: 0.4790\n",
      "Epoch 26/35\n",
      "18000/18000 [==============================] - 337s 19ms/step - loss: 0.7603 - acc: 0.4992 - val_loss: 0.7610 - val_acc: 0.4790\n",
      "Epoch 27/35\n",
      "18000/18000 [==============================] - 332s 18ms/step - loss: 0.7602 - acc: 0.5041 - val_loss: 0.7599 - val_acc: 0.5210\n",
      "Epoch 28/35\n",
      "18000/18000 [==============================] - 333s 18ms/step - loss: 0.7603 - acc: 0.5024 - val_loss: 0.7599 - val_acc: 0.5210\n",
      "Epoch 29/35\n",
      "18000/18000 [==============================] - 323s 18ms/step - loss: 0.7602 - acc: 0.5030 - val_loss: 0.7605 - val_acc: 0.4790\n",
      "Epoch 30/35\n",
      "18000/18000 [==============================] - 322s 18ms/step - loss: 0.7601 - acc: 0.4992 - val_loss: 0.7601 - val_acc: 0.4790\n",
      "Epoch 31/35\n",
      "18000/18000 [==============================] - 321s 18ms/step - loss: 0.7601 - acc: 0.5012 - val_loss: 0.7604 - val_acc: 0.4790\n",
      "Epoch 32/35\n",
      "18000/18000 [==============================] - 337s 19ms/step - loss: 0.7601 - acc: 0.4986 - val_loss: 0.7606 - val_acc: 0.4790\n",
      "Epoch 33/35\n",
      "18000/18000 [==============================] - 348s 19ms/step - loss: 0.7601 - acc: 0.4998 - val_loss: 0.7604 - val_acc: 0.4790\n",
      "Epoch 34/35\n",
      "18000/18000 [==============================] - 347s 19ms/step - loss: 0.7601 - acc: 0.5011 - val_loss: 0.7597 - val_acc: 0.5210\n",
      "Epoch 35/35\n",
      "18000/18000 [==============================] - 346s 19ms/step - loss: 0.7601 - acc: 0.4981 - val_loss: 0.7599 - val_acc: 0.4790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x130410780>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fmodel.fit(x_train, y_train , batch_size=96, epochs=35, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated CNN Result\n",
      "5000/5000 [==============================] - 17s 3ms/step\n",
      "Loss & accuracty on test set is [0.7597295370101929, 0.5038]\n"
     ]
    }
   ],
   "source": [
    "print(\"Concatenated CNN Result\")\n",
    "print(\"Loss & accuracty on test set is\", Fmodel.evaluate(x_test, y_test))\n",
    "\n",
    "#print(\"Traditional CNN Result\")\n",
    "#print(\"Loss & accuracty on test set is\", model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
